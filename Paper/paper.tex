\documentclass[11pt,a4paper]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{???}
\fi
% use upquote if available, for straight quotes in verbatim environments
%\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[lmargin=2.5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm]{geometry}

% Figure Placement:
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

%% citation setup
%\usepackage{csquotes}

\usepackage[backend=biber, maxbibnames = 99, style = apa]{biblatex}
\setlength\bibitemsep{1.5\itemsep}
\addbibresource{R_packages.bib}
\bibliography{ref.bib}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true, linktocpage = TRUE]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Alex Amaguaya, Lea Bergmann},
            pdftitle={The Gender Pay Gap in the General Social Survey},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{The Gender Pay Gap in the General Social Survey}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
\subtitle{Statistical Learning}
  \author{Alex Amaguaya, Lea Bergmann}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{31.08.2023}


%% linespread settings

\usepackage{setspace}

\onehalfspacing

% Language Setup

\usepackage{ifthen}
\usepackage{iflang}
\usepackage[super]{nth}
\usepackage[ngerman, english]{babel}

%Acronyms
\usepackage[printonlyused, withpage, nohyperlinks]{acronym}
\usepackage{changepage}

% Multicols for the Title page
\usepackage{multicol}


\usepackage{longtable}

\begin{document}

\selectlanguage{english}


%\maketitle

\begin{titlepage}
  \noindent\begin{minipage}{0.6\textwidth}
	  \IfLanguageName{english}{University of Duisburg-Essen}{Universit\"at Duisburg-Essen}\\
	  \IfLanguageName{english}{Faculty of Business Administration and Economics}{Fakult\"at f\"ur Wirtschaftswissensschaften}\\
	  \IfLanguageName{english}{Chair of Econometrics}{Lehrstuhl f\"ur \"Okonometrie}\\
  \end{minipage}
	\begin{minipage}{0.4\textwidth}
	  \begin{flushright}
  	  \vspace{-0.5cm}
      \IfLanguageName{english}{\includegraphics*[width=5cm]{Includes/duelogo_en.png}}{\includegraphics*[width=5cm]{Includes/duelogo_de.png}}
	  \end{flushright}
	\end{minipage}
  \\
  \vspace{1.5cm}
  \begin{center}
  \huge{The Gender Pay Gap in the General Social Survey}\\
  \vspace{.25cm}
  \Large{Statistical Learning}\\
  \vspace{0.5cm}
  \large{Term Paper}\\
  \vspace{1cm}
  \large{
  \IfLanguageName{english}{Submitted to the Faculty of \\ Business Administration and Economics \\at the \\University of Duisburg-Essen}{Vorgelegt der \\Fakult\"at f\"ur Wirtschaftswissenschaften der \\ Universit\"at Duisburg-Essen}\\}
  \vspace{0.75cm}
  \large{\IfLanguageName{english}{from:}{von:}}\\
  \vspace{0.5cm}
  Alex Amaguaya, Lea Bergmann\\
  \end{center}
  %\vspace{2cm}
  \vfill
  \hrulefill

  \noindent\begin{minipage}[t]{0.3\textwidth}
  \IfLanguageName{english}{Reviewer:}{Erstgutachter:}
  \end{minipage}
  \begin{minipage}[t]{0.7\textwidth}
  \hspace{1cm}
  \end{minipage}

  \noindent\begin{minipage}[t]{0.3\textwidth}
  \IfLanguageName{english}{Deadline:}{Abgabefrist:}
  \end{minipage}
  \begin{minipage}[t]{0.7\textwidth}
  \hspace{1cm}31.08.2023
  \end{minipage}

  \hrulefill

  \begin{multicols}{3}

  Name:

  Matriculation No.:

  E-Mail:

  Study Path:

  Semester:

  Graduation (est.):

  \columnbreak

 Lea Bergmann

  3104233

lea.bergmann@rwi-essen.de
lea.bergmann.1j@stud.uni-due.de

Ph.D. Economics

  \nth{1}

  Summer Term 2028

  \columnbreak

  Alex Amaguaya

  242833

  alex.amaguaya@tu-dortmund.de

  M.Sc. Econometrics

  \nth{2}

  Summer Term 2023


  \end{multicols}

\end{titlepage}

\newgeometry{top=2cm, left = 5cm, right = 2.5cm, bottom = 2.5cm}


\pagenumbering{Roman}
{
\hypersetup{linkcolor=black}

\setcounter{tocdepth}{3}
\tableofcontents
}

\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}




\begin{adjustwidth}{1.5em}{0pt}

\begin{acronym}[dummyyyy]
 \acro{ECTSCP}{European Credit Transfer System Credit Point}
 \acro{lasso}{Least Absolute Shrinkage and Selection Operator}
 \acro{pcr}{Principal Components Regression}
 \acro{pls}{Partial Least Squares}
 \acro{RMSE}{Root Mean Squared Error}
 \acroplural{LRG}[LRG]{laengefristige Refinanzierungsgeschaefte}

%Falls eine Abkuerzung in der Mehrzahl nicht einfach auf "s" endet muss das speziell eingestellt werden.
% \acro{slmtA}{super lange mega tolle Abkuerzung} %Einzahl
 %\acroplural{slmtA}[slmtAs]{super lange mega tolle Abkuerzungen} %Mehrzahl
 \acro{dummyyyy}{dummyyy}
\end{acronym}

\end{adjustwidth}

\restoregeometry

\newpage
\pagenumbering{arabic}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In this paper we predict individuals' incomes using data from the
General Social Survey (GSS, 1974-2018). The variable of interest is
\texttt{realrinc} which varies between 227\$ and
480,145\$\footnote{In constant 1986 USD.}. Hence, we can employ
regression techniques to predict individuals' incomes. Additionally, we
can consider whether \texttt{realrinc} varies by gender and if so, how
much.

The data from the GSS allow us to consider various control variables
such as gender, age, education, occupation, number of children and
marital status.

The prediction of individuals' incomes has been an issue in previous
literature. As wages are relevant to companies they want to know how to
predict wages \autocite{Chakraborti}. Some papers make use of
classification techniques to predict wages \autocite{Chakraborti}. Since
we have numeric values representing real wages and not a categorical
variable we do not make use of classification techniques but prefer
linear regression techniques such as OLS and Lasso regressions as well
as random forests. We prefer these techniques as they handle categorical
variables and outliers well, which are features of our data
\autocite{Cutler}. Wages were already predicted using random forests by
\autocite{Eichinger}. They find that using random forests results in
better predictions than using linear models. Additionally, they show
that gender barely effects the wage predictions. Likewise, the boosting
methods used by \autocite{Chakrabarty2018} in salary level
classification problems showed better performance compared to a
traditional algorithm such as support vector machines (SVM).
Furthermore, \textcite{BonaccoltoToepfer1612} suggest the use of a
penalization approach for high dimensional problems in his research
about gender wage inequality.

Based on the previous literature, we use several regression techniques
to predict \texttt{realrinc}:

\begin{itemize}
\item
  Linear Regression
\item
  Logistic Regression
\item
  Lasso, Ridge and Elastic Net Regression
\item
  Random Forest
\item
  Gradient Boosting
\item
  Generalized Additive Model
\end{itemize}

In order to evaluate which of the regression techniques predicts
\texttt{realrinc} the best we split the data set into a train (70\%) and
a test (30\%) data set. We fit the aforementioned regression models on
the train data and evaluate the root mean squared error (RMSE) using the
validation data. We choose the RMSE as it is more sensitive to large
errors \autocite{Arour}.

Furthermore, we use cross validation (CV) to assess the performance of a
model and to select appropriate hyperparameters. CV prevents overfitting
and helps in model selection.

\hypertarget{data-preparation-feature-engineering}{%
\section{Data Preparation \& Feature
Engineering}\label{data-preparation-feature-engineering}}

The initial data set contains 11 variables, which are described in the
following:

\begin{itemize}
\item
  \textbf{year}: survey year
\item
  \textbf{realrinc}: respondent's base income (in constant 1986 USD)
\item
  \textbf{age}: respondent's age in years
\item
  \textbf{occ10}: respondent's occupation code
\item
  \textbf{occrecode}: recode of the occupation code into one of 11 main
  categories
\item
  \textbf{prestg10}: respondent's occupational prestige score
\item
  \textbf{childs}: number of children
\item
  \textbf{wrkstat}: work status of the respondent (full-time, part-time,
  temporarily not working, unemployed (laid off), etc.)
\item
  \textbf{gender}: respondent's gender
\item
  \textbf{educcat}: respondent's degree level (Less Than High School,
  High School, etc.)
\item
  \textbf{maritalcat}: respondent's marital status (Married, Widowed,
  Divorced, etc.)
\end{itemize}

\hypertarget{data-preparation}{%
\subsection{Data Preparation}\label{data-preparation}}

Before modeling we conduct a data preparation process. The initial data
set has 54,842 observations and the outcome variable \texttt{realrinc}
has 38.55\% of missing values. The imputation of the target variable is
discarded to avoid generating a bias in the modeling and therefore we
eliminate these records from the original data set. After this, the data
set contains 33,702 observations. Furthermore, it is impossible to
deduce the occupation for some observations (\texttt{occ10\ =\ 9997})
and we remove these from the data set. Finally, the data set has 33,244
observations and we use it for the modeling process.

This data set still has some missing values for the regressors, e.g.,
number of children, age, marital status, degree level, and others. Thus,
we use the \texttt{mice} package with five variables for the imputation
process. The features \emph{age, occupational prestige and number of
children} are imputed using the predictive mean matching (pmm) method,
and we use the polytomous logistic regression method for the factor
variables (\emph{education and marital status}). As we use a value of 6
in the \texttt{m} parameter of the \texttt{mice} function, the
imputation process generates six different data sets. This parameter
refers to the number of imputed data sets. After this, we apply the
calculation of the mean and the majority vote to aggregate the data sets
into one. Mean estimation is applied for numerical variables and
majority voting for categorical variables

\hypertarget{feature-engineering}{%
\subsection{Feature Engineering}\label{feature-engineering}}

After the imputation process, we create interaction variables. First, we
estimate some interaction features using only the numerical variables.
Then, we transform the numerical variables
(\texttt{age,\ childs\ and\ prestg10}) into categorical variables by
separating them into ranges. The ranges created of these variables were:
\texttt{age\ between\ 18\ and\ 30,\ age\ between\ 31\ and\ 50},
\texttt{age\ greater\ than\ 50;\ childs\ between\ 0\ and\ 2,\ childs\ between\ 3\ and\ 5,\ childs\ more\ than\ 5;\ prestg10\ between\ 16\ and\ 30,\ prestg10\ between\ 31\ and\ 50,\ prestg10\ greater\ than\ 50}.
After this, we transform these new categorical variables and the initial
categorical variables (\texttt{marital\ status}, \texttt{education},
etc.) into dummy variables. Finally, we estimate the interaction
variables with the group of dummy variables mentioned above (the
interaction consists of the multiplication between two dummy variables).
The inclusion of the interactions of the numeric variables is validate
using a linear model, and the results show that the p-values of the
parameters for the interactions are below 0.05.

\begin{figure}
\centering
\includegraphics[width=3.30208in,height=\textheight]{includes/reg_table.png}
\caption{Linear Model: Inclusion of Interaction Variables}
\end{figure}

Furthermore, we estimate a correlation matrix using Spearman's rank
correlation coefficient with some numeric variables in order to support
the previous results. The correlation results are exposed in figure 2,
and show that \texttt{prestg10}, \texttt{age} and
\texttt{interaction\ between\ age\ and\ prest10} have the highest
positive correlation with \texttt{realrinc}, meanwhile the dummy
variable \texttt{female} has a moderate negative correlation with the
target variable.

\begin{figure}
\centering
\includegraphics{includes/corr_matrix.png}
\caption{Correlation Matrix}
\end{figure}

As the correlation assesses the linear relationship between two
variables and it only uses numerical features, we additionally estimate
the Predictive Power Score (PPS). It overcomes these limitations and
allows us to assess the predictive power of a regressor on a target
variable. This measure is a normalized metric and its values are between
0 (no predictive power) and 1 (perfect predictive power). The PPS is
calculated by comparing the performance of a model that uses a regressor
\texttt{x} to predict target variable \texttt{y} versus the performance
of a base model (naive). When the target variable is a numerical feature
it is common to use a Decision Tree Regressor to predict \texttt{y}
based on \texttt{x}, while in categorical cases a Decision Tree
Classifier is used. On the other hand, base models are often defined as
mean, random or modal prediction for regression and classification
problems, respectively. The PPS results are presented in figure 3, and
show that \texttt{prestg10}, \texttt{wrkstat}, \texttt{educcat} and
\texttt{occrecode} have the highest predictive power of
\texttt{realrinc} relative to the other features. These results provide
some insights in the data exploration process, but we must beware of the
limitations of this approach.

\begin{figure}
\centering
\includegraphics{includes/pps.png}
\caption{PPS Results}
\end{figure}

After the data preparation and feature engineering processes, we have a
final data set with 33,244 observations and 421 features. Due to the
large number of interaction variables, we reduce them and use only the
interactions among a selected group of variables (\texttt{occrecode},
\texttt{educcat} and categorical variable of \texttt{prestg10}). After
this, we develop the modeling process with 100 variables (numerical and
interaction variables). Figure 4 shows the overview of the group of
estimated variables that are used in the models.

\begin{figure}
\centering
\includegraphics{includes/feature_process.png}
\caption{Feature Engineering}
\end{figure}

\hypertarget{model-training-and-tuning}{%
\section{Model Training and Tuning}\label{model-training-and-tuning}}

We use five different regression techniques to predict the individuals'
wages using the final data set. We fit every model on our training data
using the \texttt{train} function from the \texttt{caret} package. To
choose the best parameter combination we use cross-validation within all
train functions.

\hypertarget{linear-and-logistic-regression}{%
\subsection{Linear and Logistic
Regression}\label{linear-and-logistic-regression}}

Our baseline linear model uses an OLS regression including all
interaction variables. This results in an RMSE of 25,506.05.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"realrinc \textasciitilde{}"}\NormalTok{, }
                            \FunctionTok{paste}\NormalTok{(x\_cols\_dummys, }\AttributeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{mod\_full }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula,}
                  \AttributeTok{data =}\NormalTok{ train, }
                  \AttributeTok{method =} \StringTok{"lm"}\NormalTok{,  }
                  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Given some extreme outliers in the income distribution of our data
(which is typical for wage data) we follow the economic literature
\autocite{Ermini} and fit a logistic model to our data, using the log of
\texttt{realrinc} as our dependent variable. This results in a higher
RMSE than the linear model (25,909.82). Still, the linear model yields
predictions of \texttt{realrinc} that are negative, what is implausible.
Using the log of \texttt{realrinc} only positive predictions are
generated. Therefore, we prefer using the log for the following
predictions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula2 }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"log\_realrinc \textasciitilde{}"}\NormalTok{, }
                              \FunctionTok{paste}\NormalTok{(x\_cols\_dummys, }\AttributeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{mod\_loginc }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula2,}
                    \AttributeTok{data =}\NormalTok{ train, }
                    \AttributeTok{method =} \StringTok{"lm"}\NormalTok{,  }
                    \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In both models the coefficient of female is statistically significant
with p \textless{} 0.001. It suggests that being female decreases the
real income by approximately 10,192\$ in the linear model or by 40\% in
the logistic model.

\hypertarget{subset-selection}{%
\subsection{Subset Selection}\label{subset-selection}}

As the data set contains many variables (361) and even our selection of
dummy variables for the right-hand side of the regression leaves us with
53 variables, we make use of regression techniques for subset selection.
We use lasso, ridge and elastic net regressions. These methods allow us
to fit a model that contains all variables. Then the model regularizes
the coefficient estimates and shrinks them towards zero. In effect, we
reduce the estimates' variance which improves the model fit.

We start with a ridge regression.

\[
y = RSS + \lambda \sum^p _{j=1} \beta^2_j 
\]

Hence, the coefficient estimates in a ridge regression minimize the sum
of the residual sum of squares (RSS) and the sum of squared coefficients
multiplied with a tuning parameter \(\lambda\). We estimate \(\lambda\)
using CV.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train,}
  \AttributeTok{method =} \StringTok{"glmnet"}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{3}\NormalTok{),}
  \AttributeTok{tuneGrid =} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.001}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Performing a ridge regression to predict real incomes yields an RMSE of
26,062.58.

In contrast to other subset selection methods ridge regression does not
exclude any variables from the regression model. Hence, the model is
still fitted on 53 covariates. To overcome this issue we also use a
lasso regression.

\[
RSS + \lambda \sum^p_{j=1} | \beta_j |
\]

In contrast to the ridge regression the lasso regression forces some of
the coefficient estimates to be equal to zero if \(\lambda\) is
sufficiently large. Hence, the regression is not performed on all
covariates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train, }
       \AttributeTok{method =} \StringTok{"glmnet"}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{), }
       \AttributeTok{tuneGrid =} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{1}\NormalTok{, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.001}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

To perform the lasso regression in \texttt{R} we set \(\alpha\) equal to
one. The lasso regression yields a lower RMSE than the ridge regression
(25,918.25 vs.~26,062.58).

In a final step we use elastic net regularization which is a linear
combination of ridge and lasso regression. The elastic net
regularization allows \(\alpha\) to vary between zero and one. This
results in an RMSE of 25,918.81.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elasticnet }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train,}
       \AttributeTok{method =} \StringTok{"glmnet"}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{),}
       \AttributeTok{tuneGrid =} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{alpha =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from=}\DecValTok{0}\NormalTok{, }\AttributeTok{to=}\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.1}\NormalTok{),}
       \AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from=}\DecValTok{0}\NormalTok{, }\AttributeTok{to=}\FloatTok{0.15}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.001}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

According to the literature review, tree-based models perform
considerably well in predicting income ranges (classification problems)
\autocite{Chakrabarty2018}. For this reason, we use the random forest
algorithm for modeling. For this method we use income without logarithm
transformation, initial variables and some interaction variables. Due to
the amount of hyperparameters in random forest, we use the
\texttt{lightgbm} and \texttt{mlr3} packages to model and tune the
hyperparameter in a time-optimal way. We tune the hyperparameters
\texttt{max\_depth}, \texttt{num\_leaves}, \texttt{num\_iterations},
etc. to find the best hyperparameters that produce the lowest error. The
search space of the hyperparameters is showed below. In addition, we use
a random search approach in the tuning process with a CV of 3 folds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tnr\_rdgrid\_search }\OtherTok{=} \FunctionTok{tnr}\NormalTok{(}\StringTok{"random\_search"}\NormalTok{, }\AttributeTok{batch\_size =} \DecValTok{10}\NormalTok{)}
\NormalTok{rsmp\_cv3 }\OtherTok{=} \FunctionTok{rsmp}\NormalTok{(}\StringTok{"cv"}\NormalTok{, }\AttributeTok{folds =} \DecValTok{3}\NormalTok{)}
\NormalTok{msr\_ce }\OtherTok{=} \FunctionTok{msr}\NormalTok{(}\StringTok{"regr.rmse"}\NormalTok{)}
\NormalTok{learner }\OtherTok{=} \FunctionTok{lrn}\NormalTok{(}\StringTok{"regr.lightgbm"}\NormalTok{,}
              \AttributeTok{boosting =} \StringTok{"rf"}\NormalTok{,}
              \AttributeTok{objective =} \StringTok{"regression"}\NormalTok{,}
              \AttributeTok{max\_depth =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{num\_leaves =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{270}\NormalTok{,}\DecValTok{280}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{num\_iterations  =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{132}\NormalTok{,}\DecValTok{137}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{min\_data\_in\_leaf =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{70}\NormalTok{,}\DecValTok{85}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{min\_data\_in\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
              \AttributeTok{feature\_fraction\_bynode =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{bagging\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{bagging\_freq =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{feature\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.7}\NormalTok{,}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{convert\_categorical =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{force\_col\_wise =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{verbose =} \DecValTok{1}\NormalTok{,}
              \AttributeTok{num\_threads =} \DecValTok{5}
\NormalTok{)}

\NormalTok{instance.rf }\OtherTok{=} \FunctionTok{tune}\NormalTok{(}
  \AttributeTok{tuner =}\NormalTok{ tnr\_rdgrid\_search,}
  \AttributeTok{task =}\NormalTok{ task,}
  \AttributeTok{learner =}\NormalTok{ learner,}
  \AttributeTok{resampling =}\NormalTok{ rsmp\_cv3,}
  \AttributeTok{measures =}\NormalTok{ msr\_ce,}
  \AttributeTok{term\_evals =} \DecValTok{300}\NormalTok{,}
  \AttributeTok{store\_models =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After identifying the model along with the hyperparameters with lowest
error in CV, we proceed to evaluate this model on the test data set and
obtain an RMSE of 31,745.54. The hyperparameters of the best model are
shown in figure 5.

\begin{figure}
\includegraphics[width=0.5\linewidth]{includes/RF_hyper} \caption{Best Hyperparameters for Random Forest}\label{fig:unnamed-chunk-1}
\end{figure}

In addition, we analyze the features importance within the modeling
process using random forest. These results are exposed in the figure 6.

\begin{figure}
\centering
\includegraphics{includes/RF_importance.png}
\caption{Top 20 of Features Importance for Random Forest}
\end{figure}

Unlike the linear models presented previously, random forest does not
predict negative values when logarithmic transformation is not used.
Still, we additionally use a random forest with the logarithmic
transformation in the target variable and keep the same configuration
(CV of 3 folds and random search approach in the tuning process).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{learner.wlog }\OtherTok{=} \FunctionTok{lrn}\NormalTok{(}\StringTok{"regr.lightgbm"}\NormalTok{,}
              \AttributeTok{boosting =} \StringTok{"rf"}\NormalTok{,}
              \AttributeTok{objective =} \StringTok{"regression"}\NormalTok{,}
              \AttributeTok{max\_depth =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{num\_leaves =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{120}\NormalTok{,}\DecValTok{140}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{max\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{45}\NormalTok{,}\DecValTok{60}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{num\_iterations  =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{90}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{min\_data\_in\_leaf =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{1}\NormalTok{)), }
              \AttributeTok{min\_data\_in\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{35}\NormalTok{,}\DecValTok{50}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
              \AttributeTok{feature\_fraction\_bynode =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.7}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{bagging\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.6}\NormalTok{,}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{bagging\_freq =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{feature\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.8}\NormalTok{,}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{convert\_categorical =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{force\_col\_wise =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{verbose =} \DecValTok{1}\NormalTok{,}
              \AttributeTok{num\_threads =} \DecValTok{5}\NormalTok{,}
              \AttributeTok{seed =} \DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The best model using the logarithmic transformation has different
hyperparameters relative to the previous experiment and obtains an RMSE
of 34,293.33 on the test data set. Thus, the error of this model is
greater than the best model without the transformation (31,745.54).

\hypertarget{gradient-boosting}{%
\subsection{Gradient Boosting}\label{gradient-boosting}}

Gradient Boosting has a similar amount of hyperparameters as random
forest and we use a similar configuration as the previous algorithm to
find the best model with the hyperparameters (random search approach in
the tuning process with 3-fold CV). The first model with Gradient
Boosting uses the target variable without logarithmic transformation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tnr\_rdgrid\_search }\OtherTok{=} \FunctionTok{tnr}\NormalTok{(}\StringTok{"random\_search"}\NormalTok{, }\AttributeTok{batch\_size =} \DecValTok{10}\NormalTok{)}
\NormalTok{rsmp\_cv3 }\OtherTok{=} \FunctionTok{rsmp}\NormalTok{(}\StringTok{"cv"}\NormalTok{, }\AttributeTok{folds =} \DecValTok{3}\NormalTok{)}
\NormalTok{msr\_ce }\OtherTok{=} \FunctionTok{msr}\NormalTok{(}\StringTok{"regr.rmse"}\NormalTok{)}
\NormalTok{learner.gb }\OtherTok{=} \FunctionTok{lrn}\NormalTok{(}\StringTok{"regr.lightgbm"}\NormalTok{,}
                 \AttributeTok{boosting =} \StringTok{"gbdt"}\NormalTok{,}
                 \AttributeTok{objective =} \StringTok{"regression"}\NormalTok{,}
                 \AttributeTok{max\_depth =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{num\_leaves =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{min\_data\_in\_leaf =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{39}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{min\_data\_in\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{feature\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{feature\_fraction\_bynode =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{learning\_rate =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.03}\NormalTok{, }\FloatTok{0.04}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}
                 \AttributeTok{num\_iterations  =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{39}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{lambda\_l1 =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{lambda\_l2 =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.30}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}
                 \AttributeTok{convert\_categorical =} \ConstantTok{TRUE}\NormalTok{,}
                 \AttributeTok{force\_col\_wise =} \ConstantTok{TRUE}\NormalTok{,}
                 \AttributeTok{verbose =} \DecValTok{1}\NormalTok{,}
                 \AttributeTok{num\_threads =} \DecValTok{5}
\NormalTok{)}

\NormalTok{instance.gb }\OtherTok{=} \FunctionTok{tune}\NormalTok{(}
  \AttributeTok{tuner =}\NormalTok{ tnr\_rdgrid\_search,}
  \AttributeTok{task =}\NormalTok{ task,}
  \AttributeTok{learner =}\NormalTok{ learner.gb,}
  \AttributeTok{resampling =}\NormalTok{ rsmp\_cv3,}
  \AttributeTok{measures =}\NormalTok{ msr\_ce,}
  \AttributeTok{term\_evals =} \DecValTok{300}\NormalTok{,}
  \AttributeTok{store\_models =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Our best model uses a combination of hyperparamters that produces the
least error. We adjust some hyperparameters such as \texttt{max\_depth},
\texttt{num\_leaves}, \texttt{min\_data\_in\_leaf},
\texttt{feature\_fraction} and others. We evaluate the best model with
the test data set and obtain an RMSE of 29,207.55. The hyperparameters
of the best model are shown in figure 7.

\begin{figure}
\includegraphics[width=0.5\linewidth]{includes/GB_hyper} \caption{Best Hyperparameters for Gradient Boosting}\label{fig:unnamed-chunk-2}
\end{figure}

Gradient Boosting also provides information on the importance of
features during the modeling process in this experiment. Figure 8 shows
the 20 most important variables according to the Gini metric.
\texttt{Gender} is after work status the most important variable, giving
additional evidence for a gender pay gap.

\begin{figure}
\centering
\includegraphics{includes/GB_importance.png}
\caption{Top 20 of Features Importance for Gradient Boosting}
\end{figure}

Furthermore, we estimate a gradient boosting model using the logarithm
transformation on the target variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{learner.gb.wlog }\OtherTok{=} \FunctionTok{lrn}\NormalTok{(}\StringTok{"regr.lightgbm"}\NormalTok{,}
                 \AttributeTok{boosting =} \StringTok{"gbdt"}\NormalTok{,}
                 \AttributeTok{objective =} \StringTok{"regression"}\NormalTok{,}
                 \AttributeTok{max\_depth =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{max\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{35}\NormalTok{,}\DecValTok{60}\NormalTok{,}\DecValTok{1}\NormalTok{)), }
                 \AttributeTok{num\_leaves =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{19}\NormalTok{,}\DecValTok{30}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{min\_data\_in\_leaf =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DecValTok{50}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{min\_data\_in\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{feature\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.8}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{feature\_fraction\_bynode =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.7}\NormalTok{,}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{learning\_rate =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }
                 \AttributeTok{num\_iterations  =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{60}\NormalTok{,}\DecValTok{80}\NormalTok{,}\DecValTok{1}\NormalTok{)), }
                 \AttributeTok{lambda\_l1 =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}
                 \AttributeTok{lambda\_l2 =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.45}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}
                 \AttributeTok{convert\_categorical =} \ConstantTok{TRUE}\NormalTok{,}
                 \AttributeTok{force\_col\_wise =} \ConstantTok{TRUE}\NormalTok{,}
                 \AttributeTok{verbose =} \DecValTok{1}\NormalTok{,}
                 \AttributeTok{num\_threads =} \DecValTok{5}\NormalTok{,}
                 \AttributeTok{seed =} \DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The best model using the logarithmic transformation has an RMSE of
34,385.47 on the test data set. Thus, the error of this model is greater
relative to the best model without the transformation (29,207.55). These
results show that in tree-based methods, Gradient Boosting has a
superior performance over Random Forest. In other words, the error of
Gradient Boosting is about 8\% smaller than the error of Random Forest
when the logarithmic transformation is not used. Nevertheless, the
errors of these models are larger than those of the linear models.

\hypertarget{generalized-additive-model}{%
\subsection{Generalized Additive
Model}\label{generalized-additive-model}}

Generalized additive models (GAM) could be considered the next step of
the linear model because they allow combining nonlinear functional forms
of the features along with a linear part. This approach is based on
relating the nonlinear predictors with a link function \(g(.)\) of the
expected values \(E(y)\). The following formulas offer more details
about this approach.

\[
y \sim \text{ExpoFam}(\mu, \text{etc.})
\] \[
E(y) = \mu
\] \[
g(\mu) = b_0 + f(x_1) + f(x_2) + \dots + f(x_p)
\] The main difference with Generalized Lineal Models (GLM) is that this
approach allows linear features to aggregate smooth functions \(f(.)\)
of the required variables and thus establish nonlinear relationships
between regressors and the dependent variable. We use the \texttt{gam}
and \texttt{mlr3} packages to model the GLM. The adjusted hyperparameter
is \texttt{gamma} and is responsible for producing a smoother term. An
additional penalization on the smooth term is aggregated with the
parameter \texttt{select}. In addition, we use a 3-fold CV in the
training stage for this algorithm. We use the logarithmic transformation
of the target variable, the interaction variables, smooth terms for the
features \texttt{age} and \texttt{prestg10} with cubic regression
splines \texttt{cr} and a value of 9 for the knots \texttt{k} (number of
sections into which a feature is divided). The smooth tensor product for
the variables \texttt{age}, \texttt{prestg10} and \texttt{child} uses a
thin-plate regression spline \texttt{tp} and a knot value of 1. The last
smooth term is considered as the interaction between two features to
predict the target variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{form.str.gam }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"log\_realrinc \textasciitilde{} s(age, bs = \textquotesingle{}cr\textquotesingle{}, k = 9) }
\StringTok{                                    + s(prestg10, bs = \textquotesingle{}cr\textquotesingle{}, k=9) + }
\StringTok{                        ti(age,prestg10) + ti(age,childs) +"}\NormalTok{,}
                       \FunctionTok{paste}\NormalTok{( x\_cols\_dummys, }\AttributeTok{collapse =} \StringTok{" + "}\NormalTok{) )}
\NormalTok{form.format.gam }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(form.str.gam)}
\NormalTok{learner.gam.wlog.smooth.tensor }\OtherTok{=} \FunctionTok{lrn}\NormalTok{(}\StringTok{"regr.gam"}\NormalTok{,}
                                     \AttributeTok{family =} \StringTok{\textquotesingle{}gaussian\textquotesingle{}}\NormalTok{,}
                                     \AttributeTok{select =} \ConstantTok{TRUE}\NormalTok{,}
                                     \AttributeTok{gamma =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\FloatTok{0.5}\NormalTok{)),}
                                     \AttributeTok{formula =}\NormalTok{ form.format.gam}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The best model obtaines an RMSE of 25,856.76 on the test data set. Thus,
the error of this model is slightly lower in relation to the linear
model that also uses the logarithmic transformation (25,909.82). In
other words, it performs 0.20\% better than the initial linear model.
Figure 9 exposes the approximate significance of the smooth terms, and
the results show that all the smooth terms used in the model have a
p-value of less than 0.01.

\begin{figure}
\includegraphics[width=0.8\linewidth]{includes/aprox_smooth} \caption{Approximate Significance of Smooth Terms}\label{fig:unnamed-chunk-3}
\end{figure}

\hypertarget{evaluation}{%
\section{Evaluation}\label{evaluation}}

The aim of this paper is to predict individuals' wages. In order to
evaluate the best prediction method we use the RMSE. The following table
demonstrates the RMSE for each used method: \medskip

\begin{tabular}{l c} 
\textbf{Method} & \textbf{RMSE} \\ 
Linear Regression & 25,506.05 \\ 
Logistic Regression & 25,909.82 \\ 
Ridge & 26,062.58 \\ 
Lasso & 25,918.25 \\
Elastic Net & 25,918.81 \\ 
Random Forest & 31,745.54 \\ 
Gradient Boosting & 29,207.55 \\ 
Generalized Additive Model & 25,856.76 \\
\end{tabular}

\medskip

Conclusively, the linear regression yields the lowest RMSE and thus
appears to be the best prediction method. Still, it delivers negative
predictions of the wage. Therefore, we prefer the logistic regression.
Comparing the RMSE of the logistic model to the one of the GAM, the GAM
(which presents a combination of linear and nonlinear functional forms
of the features) produces the lowest RMSE. Hence, the GAM presents the
best prediction method for real wages (in our data set).

Moreover, the different prediction methods consider \texttt{gender} as
an important determinant for predicting the individuals' real wage.
Still, as we do not use any causal methods other unobserved factors
might influence the wage more than \texttt{gender}.

\pagebreak

\addcontentsline{toc}{section}{References}
\printbibliography[title = References]
\cleardoublepage

\begin{refsection}
\nocite{R-base}
\nocite{R-Studio}


\end{refsection}

\end{document}